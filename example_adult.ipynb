{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py38/lib/python3.8/site-packages/sklearn/datasets/_openml.py:932: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n",
      "  warn(\n",
      "/tmp/ipykernel_3713956/3404423563.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col] = le.fit_transform(data[col])\n",
      "/tmp/ipykernel_3713956/3404423563.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col] = le.fit_transform(data[col])\n",
      "/tmp/ipykernel_3713956/3404423563.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col] = le.fit_transform(data[col])\n",
      "/tmp/ipykernel_3713956/3404423563.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col] = le.fit_transform(data[col])\n",
      "/tmp/ipykernel_3713956/3404423563.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col] = le.fit_transform(data[col])\n",
      "/tmp/ipykernel_3713956/3404423563.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col] = le.fit_transform(data[col])\n",
      "/tmp/ipykernel_3713956/3404423563.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col] = le.fit_transform(data[col])\n",
      "/tmp/ipykernel_3713956/3404423563.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col] = le.fit_transform(data[col])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "import numpy as np\n",
    "from types import SimpleNamespace\n",
    "from typing import Tuple, List\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "    \n",
    "adult = fetch_openml(data_id = 1590, data_home='./data_cache')\n",
    "\n",
    "data = adult.data\n",
    "\n",
    "le = LabelEncoder()\n",
    "label = pd.Series(le.fit_transform(adult.target))\n",
    "\n",
    "\n",
    "category_cols = ['workclass', 'education', 'race', 'sex', \"marital-status\", \"occupation\", \"relationship\", \"native-country\"]\n",
    "continuous_cols = [x for x in data.columns if x not in category_cols]\n",
    "\n",
    "for col in category_cols:\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "temp = None\n",
    "for col in category_cols:\n",
    "    oh_values = OneHotEncoder().fit_transform(data[col].values.reshape((-1, 1))).toarray()\n",
    "    new_cols = [col + \"-\" + str(i) for i in range(len(data[col].unique()))]\n",
    "    oh_values = pd.DataFrame(oh_values, columns = new_cols, dtype=np.int8, index=data.index)\n",
    "    if temp is None:\n",
    "        temp = oh_values\n",
    "    else:\n",
    "        temp = temp.merge(oh_values, left_index=True, right_index=True)\n",
    "\n",
    "data = data.merge(temp, left_index=True, right_index=True)\n",
    "data.drop(category_cols, inplace=True, axis=1)\n",
    "\n",
    "category_cols = temp.columns\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "data[continuous_cols] = scaler.fit_transform(data[continuous_cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "model_hparams = {\n",
    "    \"encoder_dim\" : data.shape[1],\n",
    "    \"predictor_hidden_dim\" : 256,\n",
    "    \"predictor_output_dim\" : 2,\n",
    "    'alpha1' : 0.5,\n",
    "    'alpha2' : 0.5,\n",
    "    'beta' : 0.5,\n",
    "    'K' : 10\n",
    "}\n",
    "data_hparams = {\n",
    "    \"K\" : 10,\n",
    "    \"p_m\" : 0.2\n",
    "}\n",
    "optim_hparams = {\n",
    "    \"lr\" : 0.005\n",
    "}\n",
    "scheduler_hparams = {\n",
    "    'gamma' : 0.3,\n",
    "    'step_size' : 30\n",
    "}\n",
    "num_categoricals = len(continuous_cols)\n",
    "num_continuous = len(continuous_cols)\n",
    "loss_fn = nn.CrossEntropyLoss\n",
    "metric =  \"accuracy_score\"\n",
    "metric_params = {}\n",
    "random_seed = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from misc.scorer import BaseScorer\n",
    "\n",
    "\n",
    "class AccuracyScorer(BaseScorer):\n",
    "    def __init__(self, metric: str) -> None:\n",
    "        super().__init__(metric)\n",
    "    \n",
    "    def __call__(self, y, y_hat) -> float:\n",
    "        return self.metric(y, y_hat.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "from pl_vime import PLVIME\n",
    "pl_vime = PLVIME(model_hparams, \"Adam\", optim_hparams, \"StepLR\", scheduler_hparams, \n",
    "       num_categoricals, num_continuous, -1, loss_fn,\n",
    "       AccuracyScorer(\"accuracy_score\"), random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(data, label, train_size = 0.7, random_state=random_seed, stratify=label)\n",
    "\n",
    "X_train, X_unlabeled, y_train, _ = train_test_split(X_train, y_train, train_size = 0.1, random_state=random_seed, stratify=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from data_utils import *\n",
    "\n",
    "n_gpus = 1\n",
    "n_jobs = 32\n",
    "max_epochs = 20\n",
    "batch_size = 512\n",
    "\n",
    "pretraining_patience = 10\n",
    "early_stopping_patience = 30\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "def fit_model(\n",
    "            model,\n",
    "    ):\n",
    "    \n",
    "    train_ds = VIMESelfDataset(X_train.append(X_unlabeled), data_hparams, continuous_cols, category_cols)\n",
    "    test_ds = VIMESelfDataset(X_valid, data_hparams, continuous_cols, category_cols)\n",
    "    \n",
    "    pl_datamodule = PLDataModule(train_ds, test_ds, batch_size=batch_size)\n",
    "\n",
    "    model.do_pretraining()\n",
    "\n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor= 'val_loss', \n",
    "            mode = 'min',\n",
    "            patience = pretraining_patience,\n",
    "            verbose = False\n",
    "        )\n",
    "    ]\n",
    "    pretraining_path = f'temporary_ckpt_data/pretraining'\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_loss',\n",
    "        dirpath=pretraining_path,\n",
    "        filename='pretraining-{epoch:02d}-{val_f1:.4f}',\n",
    "        save_top_k=1,\n",
    "        mode = 'min'\n",
    "    )\n",
    "\n",
    "    callbacks.append(checkpoint_callback)\n",
    "\n",
    "    trainer = Trainer(\n",
    "                    devices = n_gpus,\n",
    "                    accelerator=\"cuda\" if n_gpus >= 1 else 'cpu',\n",
    "                    # replace_sampler_ddp=False,\n",
    "                    max_epochs = max_epochs,\n",
    "                    num_sanity_val_steps = 2,\n",
    "                    callbacks = callbacks,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, pl_datamodule)\n",
    "    \n",
    "    pretraining_path = checkpoint_callback.best_model_path\n",
    "\n",
    "    model = model.load_from_checkpoint(pretraining_path)\n",
    "\n",
    "    model.do_finetuning()\n",
    "    \n",
    "        \n",
    "    train_ds = VIMEClassificationDataset(X_train, y_train.values, data_hparams, X_unlabeled, continuous_cols, category_cols)\n",
    "    test_ds = VIMEClassificationDataset(X_valid, y_valid.values, data_hparams, None, continuous_cols, category_cols)\n",
    "\n",
    "    pl_datamodule = PLDataModule(train_ds, test_ds, batch_size = batch_size)\n",
    "        \n",
    "    callbacks = [\n",
    "        EarlyStopping(\n",
    "            monitor= 'val_' + metric, \n",
    "            mode = 'max',\n",
    "            patience = early_stopping_patience,\n",
    "            verbose = False\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    checkpoint_path = None\n",
    "\n",
    "    checkpoint_path = f'temporary_ckpt_data/'\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor='val_' + metric,\n",
    "        dirpath=checkpoint_path,\n",
    "        filename='{epoch:02d}-{val_f1:.4f}',\n",
    "        save_top_k=1,\n",
    "        mode = 'max'\n",
    "    )\n",
    "\n",
    "    callbacks.append(checkpoint_callback)\n",
    "\n",
    "    trainer = Trainer(\n",
    "                    devices = n_gpus,\n",
    "                    accelerator = \"cuda\" if n_gpus >= 1 else 'cpu',\n",
    "                    # replace_sampler_ddp=False,\n",
    "                    max_epochs = max_epochs,\n",
    "                    num_sanity_val_steps = 2,\n",
    "                    callbacks = callbacks,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, pl_datamodule)\n",
    "\n",
    "    model = model.load_from_checkpoint(checkpoint_callback.best_model_path)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3713956/3480964513.py:19: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  train_ds = VIMESelfDataset(X_train.append(X_unlabeled), data_hparams, continuous_cols, category_cols)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/opt/conda/envs/py38/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /workspace/vime/temporary_ckpt_data/pretraining exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n",
      "\n",
      "  | Name                      | Type             | Params\n",
      "---------------------------------------------------------------\n",
      "0 | model                     | VIME             | 129 K \n",
      "1 | pretraining_mask_loss     | BCELoss          | 0     \n",
      "2 | pretraining_feature_loss1 | CrossEntropyLoss | 0     \n",
      "3 | pretraining_feature_loss2 | MSELoss          | 0     \n",
      "4 | consistency_loss          | MSELoss          | 0     \n",
      "5 | loss_fn                   | CrossEntropyLoss | 0     \n",
      "---------------------------------------------------------------\n",
      "129 K     Trainable params\n",
      "0         Non-trainable params\n",
      "129 K     Total params\n",
      "0.518     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    }
   ],
   "source": [
    "pl_vime = fit_model(pl_vime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting DataLoader 0: 100%|██████████| 29/29 [00:00<00:00, 476.09it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "trainer = Trainer(\n",
    "                    devices = n_gpus,\n",
    "                    accelerator = \"cuda\" if n_gpus >= 1 else 'cpu',\n",
    "                    max_epochs = max_epochs,\n",
    "                    num_sanity_val_steps = 2,\n",
    "                    callbacks = None,\n",
    "    )\n",
    "test_ds = VIMEClassificationDataset(X_valid, y_valid.values, data_hparams, None, continuous_cols, category_cols)\n",
    "test_dl = DataLoader(test_ds, batch_size, shuffle=False, sampler = SequentialSampler(test_ds), num_workers=n_jobs)\n",
    "\n",
    "preds = trainer.predict(pl_vime, test_dl)\n",
    "\n",
    "preds = F.softmax(torch.concat([out.cpu() for out in preds]).squeeze(),dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.706954207329557"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(y_valid, preds.argmax(1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
